---
title: loraæ¨¡å‹åŠ è½½
date: 2024-11-11 17:14:18
tags:
  - kubeflow
---

é’ˆå¯¹ M1 Mac ç”¨æˆ·ï¼Œå¦‚æœä½ ä¸æƒ³å†™ Python ä»£ç ï¼Œè€Œæ˜¯æƒ³ç”¨ç±»ä¼¼ Linux å‘½ä»¤è¡Œå·¥å…·ï¼ˆCLIï¼‰çš„æ–¹å¼æ¥èŠå¤©ï¼Œæˆ‘æœ‰ä¸¤ä¸ªæœ€ä½³æ¨èï¼š

1. **Apple MLX (æœ€æ¨è M1/M2/M3)**ï¼šApple å®˜æ–¹å‡ºçš„ AI æ¡†æ¶ï¼Œä¸“é—¨ä¸º Mac èŠ¯ç‰‡ä¼˜åŒ–ï¼Œé€Ÿåº¦æœ€å¿«ï¼Œè€Œä¸”æœ‰ç°æˆçš„å‘½ä»¤è¡Œå·¥å…·ã€‚
2. **Ollama (ä½“éªŒæœ€å¥½)**ï¼šç›®å‰æœ€æµè¡Œçš„å‘½ä»¤è¡Œ AI å·¥å…·ï¼Œä½†éœ€è¦å…ˆè¿›è¡Œâ€œæ¨¡å‹åˆå¹¶â€çš„æ“ä½œã€‚

é‰´äºä½ æ˜¯ DevOps èƒŒæ™¯ï¼Œæˆ‘å¼ºçƒˆå»ºè®®èµ° **â€œåˆå¹¶æ¨¡å‹ -> Ollamaâ€** è¿™æ¡è·¯ã€‚è¿™æ˜¯æœ€æ ‡å‡†çš„ç”Ÿäº§åŒ–æµç¨‹ã€‚

------

### æ–¹æ¡ˆä¸€ï¼šä½¿ç”¨ Ollama (æœ€æ ‡å‡†çš„ DevOps æ–¹å¼)

Ollama ä¸èƒ½ç›´æ¥è¯»å–åˆ†ç¦»çš„ LoRA æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆè¿è¡Œä¸€ä¸ªè„šæœ¬ï¼ŒæŠŠ **åº•åº§æ¨¡å‹ (Base)** å’Œ **LoRA è¡¥ä¸** æ°¸ä¹…â€œç„Šæ¥â€åœ¨ä¸€èµ·ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„æ¨¡å‹æ–‡ä»¶ã€‚

#### ç¬¬ä¸€æ­¥ï¼šåˆå¹¶æ¨¡å‹ (Merge)

åœ¨ä½ çš„é¡¹ç›®ç›®å½•ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ª `merge.py` è„šæœ¬ï¼š

Python

```
import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# 1. é…ç½®è·¯å¾„ (è¯·ç¡®ä¿è¿™äº›è·¯å¾„å’Œä½ ä¹‹å‰çš„ä¸€è‡´)
base_path = os.path.expanduser("~/ai-models/qwen/Qwen1.5-0.5B-Chat")
lora_path = os.path.expanduser("~/Downloads/qwen-lora-result")
save_path = os.path.expanduser("~/ai-models/qwen-merged") # åˆå¹¶åçš„ä¿å­˜ä½ç½®

print("ğŸš€ æ­£åœ¨åŠ è½½åº•åº§æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_path, device_map="cpu", trust_remote_code=True, torch_dtype=torch.float16
)

print("ğŸ›  æ­£åœ¨åŠ è½½å¹¶åˆå¹¶ LoRA...")
model = PeftModel.from_pretrained(base_model, lora_path)
model = model.merge_and_unload() # å…³é”®æ­¥éª¤ï¼šç„Šæ¥ï¼

print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {save_path}")
model.save_pretrained(save_path)
# é¡ºä¾¿æŠŠ tokenizer ä¹Ÿæ‹·è¿‡å»ï¼ŒOllama éœ€è¦
tokenizer = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True)
tokenizer.save_pretrained(save_path)

print("âœ… åˆå¹¶å®Œæˆï¼")
```

**è¿è¡Œåˆå¹¶ï¼š**

Bash

```
./venv/bin/python merge.py
```

*è¿™ä¼šåœ¨ `~/ai-models/qwen-merged` ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ã€‚*

#### ç¬¬äºŒæ­¥ï¼šå®‰è£… Ollama

å¦‚æœä½ è¿˜æ²¡å®‰è£… Ollamaï¼š

1. å» [ollama.com](https://ollama.com/) ä¸‹è½½ Mac ç‰ˆå¹¶å®‰è£…ã€‚
2. å®‰è£…ååœ¨ç»ˆç«¯éªŒè¯ï¼š`ollama --version`

#### ç¬¬ä¸‰æ­¥ï¼šåˆ›å»º Modelfile

Ollama éœ€è¦ä¸€ä¸ªé…ç½®æ–‡ä»¶æ¥çŸ¥é“æ€ä¹ˆè¿è¡Œä½ çš„æ¨¡å‹ã€‚åœ¨ç»ˆç«¯åˆ›å»ºä¸€ä¸ªåä¸º `Modelfile` çš„æ–‡ä»¶ï¼ˆæ— åç¼€ï¼‰ï¼š

Dockerfile

```
# å‘Šè¯‰ Ollama æ¨¡å‹æ–‡ä»¶åœ¨å“ª (è¯·æ›¿æ¢ä¸ºä½ çš„çœŸå®è·¯å¾„)
FROM /Users/linchangkun/ai-models/qwen-merged

# è®¾ç½®å‚æ•° (åƒé—®æ¨èå‚æ•°)
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER stop "<|endoftext|>"
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"

# è®¾ç½®å¯¹è¯æ¨¡æ¿ (åƒé—®æ ‡å‡†æ¨¡æ¿)
TEMPLATE """<|im_start|>system
{{ .System }}<|im_end|>
<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
"""

# è®¾ç½®ç³»ç»Ÿæç¤ºè¯
SYSTEM """ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚"""
```

*æ³¨æ„ï¼š`FROM` åé¢å¿…é¡»æ˜¯ç»å¯¹è·¯å¾„ã€‚*

#### ç¬¬å››æ­¥ï¼šæ„å»ºå¹¶è¿è¡Œ

åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

Bash

```
# 1. åˆ›å»ºæ¨¡å‹ (åå­—å« qwen-lora)
ollama create qwen-lora -f Modelfile

# 2. è¿è¡Œå¯¹è¯ (è¿›å…¥ CLI èŠå¤©ç•Œé¢)
ollama run qwen-lora
```

**ç°åœ¨ï¼Œä½ å°±å¯ä»¥åœ¨æ¼‚äº®çš„å‘½ä»¤è¡Œç•Œé¢é‡Œå’Œä½ çš„æ¨¡å‹å¯¹è¯äº†ï¼** è¿˜å¯ä»¥éšæ—¶è¾“å…¥ `/bye` é€€å‡ºã€‚


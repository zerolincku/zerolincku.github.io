---
r57114c5deb2-node-0-0-xqpjhr57114c5deb2-node-0-0-xqpjhtitle: kubeflowåˆ†å¸ƒå¼è®­ç»ƒ
date: 2024-11-11 17:14:18
tags:
  - kubeflow
---

~~~python
import datetime
import logging

from kubeflow.trainer import TrainerClient
from kubeflow.trainer.types.types import CustomTrainer

logging.basicConfig(level=logging.INFO)

# ================= 1. RustFS (S3) é…ç½® =================
RUSTFS_ENDPOINT = "http://172.17.0.1:9000"
RUSTFS_ACCESS_KEY = "minio"
RUSTFS_SECRET_KEY = "password"

# è¾“å…¥é…ç½®
MODEL_BUCKET = "model"
MODEL_DIR_NAME = "Qwen1___5-0___5B-Chat"
DATASET_BUCKET = "dataset"
DATASET_FILE_NAME = "test.json"

# ã€ä¿®æ”¹ç‚¹ 1ã€‘è¾“å‡º Bucket å›ºå®šä¸º ai_train_output
# æ–‡ä»¶å¤¹å (OUTPUT_PREFIX) å°†åœ¨ submit_job ä¸­åŠ¨æ€ç”Ÿæˆ
OUTPUT_BUCKET = "ai-train-output"

# ================= 2. è®­ç»ƒé€»è¾‘ (Pod å†…è¿è¡Œ) =================
def train_with_rustfs():
    import os

    import boto3
    import torch
    from datasets import Dataset
    from peft import LoraConfig, TaskType, get_peft_model
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        Trainer,
        TrainingArguments,
    )

    # --- è·å–ç¯å¢ƒå˜é‡ ---
    endpoint = os.environ['RUSTFS_ENDPOINT']
    ak = os.environ['RUSTFS_ACCESS_KEY']
    sk = os.environ['RUSTFS_SECRET_KEY']

    # è·å–è¾“å‡ºé…ç½®
    output_bucket = os.environ['OUTPUT_BUCKET']
    output_prefix = os.environ['OUTPUT_PREFIX'] # è¿™é‡Œå°±æ˜¯ Job ID

    print(f"ğŸ“¡ [Pod] Connecting to RustFS at {endpoint}...")
    print(f"ğŸ¯ [Pod] Output Target: s3://{output_bucket}/{output_prefix}")

    # åˆå§‹åŒ– S3 å®¢æˆ·ç«¯
    s3 = boto3.client('s3',
        endpoint_url=endpoint,
        aws_access_key_id=ak,
        aws_secret_access_key=sk
    )

    # --- è¾…åŠ©å‡½æ•°ï¼šä¸‹è½½æ–‡ä»¶å¤¹ ---
    def download_s3_folder(bucket, prefix, local_dir):
        print(f"â¬‡ï¸ [Pod] Downloading folder: s3://{bucket}/{prefix} -> {local_dir}")
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)
        for page in pages:
            if 'Contents' not in page: continue
            for obj in page['Contents']:
                key = obj['Key']
                if key.endswith('/'): continue
                relative_path = os.path.relpath(key, prefix)
                local_file_path = os.path.join(local_dir, relative_path)
                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
                s3.download_file(bucket, key, local_file_path)

    # --- è¾…åŠ©å‡½æ•°ï¼šä¸Šä¼ æ–‡ä»¶å¤¹ ---
    def upload_folder_to_s3(local_dir, bucket, s3_prefix):
        print(f"â¬†ï¸ [Pod] Uploading results: {local_dir} -> s3://{bucket}/{s3_prefix}")
        files_count = 0
        for root, dirs, files in os.walk(local_dir):
            for file in files:
                local_path = os.path.join(root, file)
                relative_path = os.path.relpath(local_path, local_dir)
                # æœ€ç»ˆè·¯å¾„: job-id/relative_path
                s3_key = os.path.join(s3_prefix, relative_path)

                print(f"   - Uploading: {s3_key}")
                try:
                    s3.upload_file(local_path, bucket, s3_key)
                    files_count += 1
                except Exception as e:
                    print(f"   âŒ Failed to upload {file}: {e}")
        print(f"âœ… Uploaded {files_count} files.")

    # --- æ­¥éª¤ 1: ä¸‹è½½æ¨¡å‹ ---
    local_model_path = "/tmp/model"
    model_bucket = os.environ['MODEL_BUCKET']
    model_prefix = os.environ['MODEL_DIR_NAME']
    download_s3_folder(model_bucket, model_prefix, local_model_path)

    # --- æ­¥éª¤ 2: ä¸‹è½½æ•°æ®é›† ---
    local_data_path = "/tmp/data/test.json"
    os.makedirs(os.path.dirname(local_data_path), exist_ok=True)
    ds_bucket = os.environ['DATASET_BUCKET']
    ds_file = os.environ['DATASET_FILE_NAME']
    s3.download_file(ds_bucket, ds_file, local_data_path)

    # --- æ­¥éª¤ 3: åŠ è½½æ¨¡å‹ & LoRA ---
    print(f"ğŸ“¦ [Pod] Loading Model...")
    tokenizer = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        local_model_path, torch_dtype=torch.float32, device_map="cpu", local_files_only=True, trust_remote_code=True
    )

    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, target_modules=['q_proj', 'v_proj'])
    model = get_peft_model(model, peft_config)

    # --- æ­¥éª¤ 4: å¤„ç†æ•°æ® ---
    dataset = Dataset.from_json(local_data_path)
    def process(x):
        text = x.get("text", str(x))
        inputs = tokenizer(text, padding="max_length", max_length=128, truncation=True)
        inputs["labels"] = inputs["input_ids"]
        return inputs
    tokenized_ds = dataset.map(process)

    # --- æ­¥éª¤ 5: è®­ç»ƒ ---
    print("ğŸ”¥ [Pod] Starting Training...")
    args = TrainingArguments(
        output_dir="/tmp/output",
        max_steps=20,
        use_cpu=True,
        per_device_train_batch_size=1,
        logging_steps=1,
        save_strategy="no",
        report_to="none"
    )
    trainer = Trainer(model=model, args=args, train_dataset=tokenized_ds)
    trainer.train()

    # --- æ­¥éª¤ 6: ä¿å­˜å¹¶ä¸Šä¼ ç»“æœ ---
    print("ğŸ’¾ [Pod] Saving final model locally...")
    final_save_path = "/tmp/final_model"
    trainer.save_model(final_save_path)
    tokenizer.save_pretrained(final_save_path)

    # ä¸Šä¼ åˆ° s3://ai_train_output/{job_id}/...
    upload_folder_to_s3(final_save_path, output_bucket, output_prefix)

    print("âœ… [Pod] All tasks finished!")

# ================= 3. æäº¤ä»»åŠ¡ =================
def submit_job():
    client = TrainerClient()

    # ã€ä¿®æ”¹ç‚¹ã€‘ä½¿ç”¨ yyyyMMddHHmmss ä½œä¸º S3 æ–‡ä»¶å¤¹å
    # ä¾‹å¦‚: 20260103194527
    s3_run_id = datetime.datetime.now().strftime("%Y%m%d%H%M%S")

    print(f"ğŸ†” S3 Storage ID: {s3_run_id}")
    print(f"ğŸ“‚ Output S3 Path: s3://{OUTPUT_BUCKET}/{s3_run_id}/")

    trainer = CustomTrainer(
        func=train_with_rustfs,
        packages_to_install=[
            "boto3", "transformers", "peft", "torch", "accelerate", "datasets", "tiktoken"
        ],
        num_nodes=1,
        env={
            "PET_NPROC_PER_NODE": "1",
            "OMP_NUM_THREADS": "1",
            "RUSTFS_ENDPOINT": RUSTFS_ENDPOINT,
            "RUSTFS_ACCESS_KEY": RUSTFS_ACCESS_KEY,
            "RUSTFS_SECRET_KEY": RUSTFS_SECRET_KEY,
            "MODEL_BUCKET": MODEL_BUCKET,
            "MODEL_DIR_NAME": MODEL_DIR_NAME,
            "DATASET_BUCKET": DATASET_BUCKET,
            "DATASET_FILE_NAME": DATASET_FILE_NAME,
            # å°†æ—¶é—´æˆ³æ³¨å…¥ç¯å¢ƒå˜é‡
            "OUTPUT_BUCKET": OUTPUT_BUCKET,
            "OUTPUT_PREFIX": s3_run_id,
        },
        resources_per_node={"cpu": "2", "memory": "6Gi", "gpu": "0"}
    )

    print("ğŸš€ Submitting Job...")

    # æ¸…ç†æ—§èµ„æº
    import subprocess
    subprocess.run("kubectl delete trainjob --all", shell=True)
    subprocess.run("kubectl delete pods --all --force --grace-period=0", shell=True)

    # SDK è‡ªåŠ¨ç”Ÿæˆ K8s ä»»åŠ¡ ID
    k8s_job_id = client.train(trainer=trainer)

    print("-" * 40)
    print(f"âœ… Job Submitted Successfully!")
    print(f"ğŸ·ï¸  K8s Job Name: {k8s_job_id} (ç”¨äºæŸ¥çœ‹æ—¥å¿—)")
    print(f"ğŸ“¦ S3 Output Dir: {s3_run_id} (ç”¨äºä¸‹è½½æ¨¡å‹)")
    print("-" * 40)

    print(f"ğŸ” Watch logs: python print_log.py --name {k8s_job_id}")

if __name__ == "__main__":
    submit_job()

~~~

~~~python
## print_log.py
import argparse
import logging
import time

from kubeflow.trainer import TrainerClient

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.INFO)

def main():
    parser = argparse.ArgumentParser(description="å¾ªç¯å¢é‡è·å– Kubeflow Trainer Job æ—¥å¿—")
    parser.add_argument(
        "--name",
        type=str,
        required=True,
        help="Job çš„åç§° (ä¾‹å¦‚: kd7fa8a6c9b8)"
    )
    args = parser.parse_args()
    job_name = args.name

    client = TrainerClient()
    print(f"å¼€å§‹ç›‘å¬ Job: {job_name} çš„æ—¥å¿— (å¢é‡æ¨¡å¼, æŒ‰ Ctrl+C åœæ­¢)...")

    # è®°å½•ä¸Šæ¬¡æ‰“å°åˆ°çš„è¡Œæ•°ä¸‹æ ‡
    last_line_index = 0

    while True:
        try:
            # === ä¿®æ”¹ç‚¹åœ¨è¿™é‡Œ ===
            # get_job_logs è¿”å›çš„æ˜¯ generatorï¼Œå¿…é¡»è½¬æˆ list æ‰èƒ½è·å–é•¿åº¦å’Œåˆ‡ç‰‡
            log_generator = client.get_job_logs(name=job_name)
            all_logs = list(log_generator)

            # è®¡ç®—å½“å‰æ€»è¡Œæ•°
            current_total_lines = len(all_logs)

            # åˆ¤æ–­æ˜¯å¦æœ‰æ–°æ—¥å¿—
            if current_total_lines > last_line_index:
                # è·å–ä» last_line_index å¼€å§‹çš„æ‰€æœ‰æ–°è¡Œ
                new_lines = all_logs[last_line_index:]

                # æ‰“å°æ–°è¡Œ
                if new_lines:
                    print("\n".join(new_lines))

                # æ›´æ–°æ¸¸æ ‡
                last_line_index = current_total_lines

            # å¦‚æœæ—¥å¿—å˜å°‘äº†ï¼ˆæ¯”å¦‚ Pod é‡å¯ï¼‰ï¼Œé‡ç½®æ¸¸æ ‡
            elif current_total_lines < last_line_index:
                # åªæœ‰å½“ current_total_lines > 0 æ—¶æ‰è®¤ä¸ºæ˜¯é‡å¯ï¼Œé˜²æ­¢ç½‘ç»œæ³¢åŠ¨è·å–åˆ°ç©ºåˆ—è¡¨è¯¯åˆ¤
                if current_total_lines > 0:
                    logging.warning("æ—¥å¿—è¡Œæ•°å‡å°‘ï¼Œå¯èƒ½ Pod å·²é‡å¯ï¼Œé‡æ–°è¾“å‡º...")
                    last_line_index = 0
                # å¦‚æœè·å–åˆ° 0 è¡Œï¼Œå¯èƒ½æ˜¯æš‚æ—¶æ²¡å–åˆ°ï¼Œä¿æŒ last_line_index ä¸å˜æˆ–æ ¹æ®å®é™…æƒ…å†µå¤„ç†

            time.sleep(2)

        except KeyboardInterrupt:
            print("\nåœæ­¢ç›‘å¬ã€‚")
            break
        except Exception as e:
            # æ‰“å°å…·ä½“é”™è¯¯ç±»å‹ï¼Œæ–¹ä¾¿è°ƒè¯•
            logging.error(f"è·å–æ—¥å¿—æ—¶å‘ç”Ÿé”™è¯¯: {type(e).__name__}: {e}")
            time.sleep(5)

if __name__ == "__main__":
    main()

~~~


~~~python
import logging
from kubeflow.trainer import TrainerClient
from kubeflow.trainer.types.types import CustomTrainer

logging.basicConfig(level=logging.INFO)

# ================= 1. RustFS (S3) é…ç½® =================
# å¦‚æœä½ æ˜¯ OrbStack: http://host.orb.internal:9000
# å¦‚æœä½ æ˜¯ Docker Desktop: http://host.docker.internal:9000
# å¦‚æœæ˜¯æœ¬æœºç›´æ¥è¿è¡Œ RustFSï¼Œä¹Ÿå¯ä»¥å°è¯•å±€åŸŸç½‘ IP: http://192.168.x.x:9000
RUSTFS_ENDPOINT = "http://172.18.0.6:9000"

# âš ï¸ è¯·æ›¿æ¢ä¸ºä½  RustFS çš„çœŸå®è´¦å·å¯†ç 
RUSTFS_ACCESS_KEY = "minio"
RUSTFS_SECRET_KEY = "password"

# æ•°æ®é…ç½®
MODEL_BUCKET = "model"
MODEL_DIR_NAME = "Qwen3-0___6B" # Bucket ä¸‹çš„æ–‡ä»¶å¤¹å

DATASET_BUCKET = "dataset"
DATASET_FILE_NAME = "test.json"

# ================= 2. è®­ç»ƒé€»è¾‘ (Pod å†…è¿è¡Œ) =================
def train_with_rustfs():
    import os
    import boto3
    import torch
    import json
    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset

    # è·å–ç¯å¢ƒå˜é‡
    endpoint = os.environ['RUSTFS_ENDPOINT']
    ak = os.environ['RUSTFS_ACCESS_KEY']
    sk = os.environ['RUSTFS_SECRET_KEY']

    print(f"ğŸ“¡ [Pod] start Connecting to RustFS at {endpoint}...")

    # åˆå§‹åŒ– S3 å®¢æˆ·ç«¯
    s3 = boto3.client('s3',
        endpoint_url=endpoint,
        aws_access_key_id=ak,
        aws_secret_access_key=sk
    )

    print(f"ğŸ“¡ [Pod] Connecting to RustFS at {endpoint}...")

    # --- è¾…åŠ©å‡½æ•°ï¼šä¸‹è½½æ–‡ä»¶å¤¹ ---
    def download_s3_folder(bucket, prefix, local_dir):
        print(f"â¬‡ï¸ [Pod] Downloading folder: s3://{bucket}/{prefix} -> {local_dir}")
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)

        count = 0
        for page in pages:
            if 'Contents' not in page: continue
            for obj in page['Contents']:
                key = obj['Key']
                # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿ä¸‹è½½åç»“æ„æ­£ç¡®
                # ä¾‹å¦‚ key="Qwen3/config.json", relative="config.json"
                if key.endswith('/'): continue # è·³è¿‡ç©ºæ–‡ä»¶å¤¹å¯¹è±¡

                relative_path = os.path.relpath(key, prefix)
                local_file_path = os.path.join(local_dir, relative_path)

                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
                s3.download_file(bucket, key, local_file_path)
                print(f"   - Downloaded: {relative_path}")
                count += 1
        print(f"âœ… Downloaded {count} model files.")

    # --- æ­¥éª¤ 1: ä¸‹è½½æ¨¡å‹ ---
    local_model_path = "/tmp/model"
    model_bucket = os.environ['MODEL_BUCKET']
    model_prefix = os.environ['MODEL_DIR_NAME']

    download_s3_folder(model_bucket, model_prefix, local_model_path)

    # --- æ­¥éª¤ 2: ä¸‹è½½æ•°æ®é›† ---
    local_data_path = "/tmp/data/test.json"
    os.makedirs(os.path.dirname(local_data_path), exist_ok=True)

    ds_bucket = os.environ['DATASET_BUCKET']
    ds_file = os.environ['DATASET_FILE_NAME']

    print(f"â¬‡ï¸ [Pod] Downloading dataset: s3://{ds_bucket}/{ds_file}")
    s3.download_file(ds_bucket, ds_file, local_data_path)

    # --- æ­¥éª¤ 3: åŠ è½½æ¨¡å‹ ---
    print(f"ğŸ“¦ [Pod] Loading Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(local_model_path, local_files_only=True, trust_remote_code=True)
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

    print(f"ğŸ“¦ [Pod] Loading Model (CPU Mode)...")
    model = AutoModelForCausalLM.from_pretrained(
        local_model_path,
        torch_dtype=torch.float32,
        device_map="cpu",
        local_files_only=True,
        trust_remote_code=True
    )

    print("âš™ï¸ [Pod] Applying LoRA...")
    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, target_modules=['q_proj', 'v_proj'])
    model = get_peft_model(model, peft_config)

    # --- æ­¥éª¤ 4: åŠ è½½æœ¬åœ° JSON æ•°æ®é›† ---
    print("ğŸ”„ [Pod] Loading Dataset from JSON...")
    # ä½¿ç”¨ HuggingFace Datasets åŠ è½½åˆšæ‰ä¸‹è½½çš„ JSON
    dataset = Dataset.from_json(local_data_path)

    # ç®€å•æ‰“å°ä¸€ä¸‹æ•°æ®çœ‹çœ‹
    print(f"   Data sample: {dataset[0]}")

    def process(x):
        # å‡è®¾ json é‡Œæœ‰ "text" å­—æ®µï¼Œå¦‚æœä½ çš„å­—æ®µåä¸ä¸€æ ·ï¼Œè¯·åœ¨è¿™é‡Œä¿®æ”¹
        # ä¾‹å¦‚: text = f"User: {x['instruction']} Assistant: {x['output']}"
        text = x.get("text", str(x))
        inputs = tokenizer(text, padding="max_length", max_length=128, truncation=True)

        # 3. âš ï¸ã€å…³é”®ä¿®å¤ã€‘æ·»åŠ  labels
        # å¦‚æœä¸åŠ è¿™ä¸€è¡Œï¼ŒTrainer å°±ä¼šæŠ¥ ValueError: The model did not return a loss...
        # åªæœ‰å‘Šè¯‰æ¨¡å‹ "labels" æ˜¯ä»€ä¹ˆï¼Œå®ƒæ‰çŸ¥é“æ€ä¹ˆç®— loss
        inputs["labels"] = inputs["input_ids"]

        return inputs

    tokenized_ds = dataset.map(process)

    # --- æ­¥éª¤ 5: è®­ç»ƒ ---
    print("ğŸ”¥ [Pod] Starting Training...")
    args = TrainingArguments(
        output_dir="/tmp/output",
        max_steps=1000,
        use_cpu=True,
        per_device_train_batch_size=1,

        logging_steps=1,       # å…³é”®ï¼šæ¯èµ° 1 æ­¥å°±æ‰“å°ä¸€æ¬¡ Loss
        disable_tqdm=False,    # ç¡®ä¿æ˜¾ç¤ºè¿›åº¦æ¡
        report_to="none"       # ä¸å°è¯•è¿æ¥ wandb/tensorboardï¼Œçº¯æ‰“å°
    )
    trainer = Trainer(model=model, args=args, train_dataset=tokenized_ds)
    trainer.train()
    print("âœ… [Pod] Training Completed with RustFS Data!")

# ================= 3. æäº¤ä»»åŠ¡ =================
def submit_job():
    client = TrainerClient()

    trainer = CustomTrainer(
        func=train_with_rustfs,
        # âš ï¸ å…³é”®ï¼šå¿…é¡»å®‰è£… boto3 ç”¨äºè¿æ¥ RustFS
        packages_to_install=[
            "boto3", "transformers", "peft", "torch", "accelerate", "datasets", "tiktoken"
        ],
        num_nodes=2,
        env={
            "PET_NPROC_PER_NODE": "1",
            "OMP_NUM_THREADS": "1",
            # æ³¨å…¥é…ç½®
            "RUSTFS_ENDPOINT": RUSTFS_ENDPOINT,
            "RUSTFS_ACCESS_KEY": RUSTFS_ACCESS_KEY,
            "RUSTFS_SECRET_KEY": RUSTFS_SECRET_KEY,
            "MODEL_BUCKET": MODEL_BUCKET,
            "MODEL_DIR_NAME": MODEL_DIR_NAME,
            "DATASET_BUCKET": DATASET_BUCKET,
            "DATASET_FILE_NAME": DATASET_FILE_NAME
        },
        resources_per_node={"cpu": "4", "memory": "8Gi", "gpu": "0"}
    )

    print("ğŸš€ Submitting Job (RustFS Mode)...")

    # æ¸…ç†æ—§èµ„æº
    import subprocess
    subprocess.run("kubectl delete trainjob --all", shell=True)
    subprocess.run("kubectl delete pods --all --force --grace-period=0", shell=True)

    job_id = client.train(trainer=trainer)
    print(f"âœ… Job Submitted! ID: {job_id}")
    print(f"ğŸ” Watch logs: kubectl logs -n kubeflow-system -f {job_id}-worker-0")

if __name__ == "__main__":
    submit_job()
~~~
